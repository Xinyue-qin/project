## LITERATURE REVIEWs

**AN EXTENSIVE EVALUATION OF PDDL CAPABILITIES IN OFF-THE-SHELF LLMS (https://arxiv.org/pdf/2502.20175, by Kaustubh Vyas, Damien Graux, Sebastien Montella etc.)**
 LLMs are capable of translating natural language descriptions into syntactically valid PDDL representations. However, there are gaps in between gold-standard models and gold-standard models. Kaustubh et al. (2025) focused on seven major families, and explored their ability in reasoning capabilities and executing potential complete planning tasks from aspects of parsing, generating, and reasoning PDDL. Zero-shot prompting was applied in 13,000 (NL-instruction, PDDL-problem) pairs' experiments to ensure an unbiased assessment of the models without modifying states. The researchers pointed out that some models (e.g., ... ) demonstrate moderate proficiency in handling PDDL, while the majority (...) struggle to convert natural language instructions into fully correct PDDL representations. Some smaller LLMs like .., have disappointing performace in producting parsable PDDL.

several evaluation methods were applied to score the LLMs abilities in generating PDDL domains, problems and plans.


**Large Language Models as Planning Domain Generators**

### Exploring and Benchmarking Planning Capabilities of Large Language Models

[EXPLORING AND BENCHMARKING PLANNING CAPABILITIES OF LARGE LANGUAGE MODELS.pdf](https://github.com/user-attachments/files/21094318/EXPLORING.AND.BENCHMARKING.PLANNING.CAPABILITIES.OF.LARGE.LANGUAGE.MODELS.pdf)

benchmarks: LLM performance was assusing In-Context Learning (ICL), Supervised Fine-Tuning (SFT), and chain-of-thought (CoT) methods for planning.

On these benchmarks, we assess LLM performance using **In-Context Learning (ICL)**, **Supervised Fine-Tuning (SFT)**, and **chain-of-thought (CoT)** methods for planning.
